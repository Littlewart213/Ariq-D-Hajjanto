import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import streamlit as st
import time
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np  # Import numpy for numerical operations

# Set up the transforms
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert images to tensors
    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to the range [-1, 1]
])

# Input MNIST dataset
train_data = datasets.MNIST(root='./Data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='./Data', train=False, download=True, transform=transform)

# Sidebar for input
dataset = st.sidebar.text_input("Dataset:", value="MNIST")
model_name = st.sidebar.text_input("Model Name:", value="model-mnist")

# Main function
def main():
    st.title("FP Multimodal Biomedical Image Analysis (Ariq D. Hajjanto/5023201057)")
    menu = ["Home", "Dataset", "Artificial Neural Network"]
    choice = st.sidebar.selectbox("Menu", menu)

    if choice == "Home":
        st.subheader("Home")
        st.write("Welcome to MNIST Classification!")
    elif choice == "Artificial Neural Network":
        st.subheader("ANN - MNIST Classification")
        st.write("Training and evaluating an Artificial Neural Network on MNIST dataset.")

        torch.manual_seed(101)
        train_batch_size = int(st.sidebar.number_input("Train Batch Size:", min_value=1, max_value=600, value=64))
        test_batch_size = int(st.sidebar.number_input("Test Batch Size:", min_value=1, max_value=600, value=100))
        epochs = st.sidebar.slider('Enter Epoch Value', 1, 20, 1)
        learning_rate = st.sidebar.number_input("Learning Rate:", value=0.001)

        stop_training = False

        if st.sidebar.button("Start Training"):
            train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)
            test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False)
            model = train_model(train_loader, test_loader, epochs, learning_rate)

            # Show confusion matrix
            cm, class_labels = show_confusion_matrix(test_loader, model)
            st.write("Confusion Matrix")
            st.write(class_labels)
            st.write(cm)

        if st.sidebar.button("Stop Training"):
            stop_training = True
            st.write("Training will stop at the end of the current epoch.")

# Function to define the model architecture
class MultilayerPerceptron(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)  # Input layer: 784 (28x28) -> 128
        self.fc2 = nn.Linear(128, 64)    # Hidden layer 1: 128 -> 64
        self.fc3 = nn.Linear(64, 10)     # Output layer: 64 -> 10 (0-9 digits)

    def forward(self, x):
        x = x.view(x.shape[0], -1)      # Flatten the input tensor
        x = F.relu(self.fc1(x))         # ReLU activation for hidden layer 1
        x = F.relu(self.fc2(x))         # ReLU activation for hidden layer 2
        x = self.fc3(x)                 # Output layer
        return F.log_softmax(x, dim=1)  # Log softmax activation for output

def train_model(train_loader, test_loader, epochs, learning_rate):
    model = MultilayerPerceptron()

    # Set up the optimizer and criterion
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Track the training and testing losses
    train_losses = []
    test_losses = []

    # Track the training and testing accuracies
    train_correct = []
    test_correct = []

    # Check if a GPU is available
    train_on_gpu = torch.cuda.is_available()
    if train_on_gpu:
        model.cuda()

    start_time = time.time()

    # Create an empty placeholder for displaying progress
    progress_placeholder = st.empty()

    for epoch in range(epochs):
        model.train()
        trn_corr = 0
        for batch, (X_train, y_train) in enumerate(train_loader):
            if train_on_gpu:
                X_train, y_train = X_train.cuda(), y_train.cuda()

            optimizer.zero_grad()
            output = model(X_train)
            loss = criterion(output, y_train)
            loss.backward()
            optimizer.step()

            # Track the training loss and accuracy for each iteration
            predicted = torch.max(output.data, 1)[1]
            batch_corr = (predicted == y_train).sum()
            trn_corr += batch_corr

            # Append the losses and accuracies for each iteration
            train_losses.append(loss.item())
            train_correct.append(batch_corr.item() / train_loader.batch_size)

            # Update the progress display
            progress_placeholder.text(f"Epoch: {epoch+1}/{epochs}, Batch: {batch+1}/{len(train_loader)}, Loss: {loss.item():.6f}, Accuracy: {batch_corr.item() / train_loader.batch_size:.4f}")

        # Evaluate on the test set
        test_loss, test_acc = evaluate_model(model, test_loader, criterion, train_on_gpu)
        test_losses.append(test_loss)
        test_correct.append(test_acc)

    # Calculate average loss and accuracy
    train_loss = np.mean(train_losses)
    train_accuracy = np.mean(train_correct)
    test_loss = np.mean(test_losses)
    test_accuracy = np.mean(test_correct)

    st.write(f"\nAverage Training Accuracy: {train_accuracy:.4f}")
    st.write(f"Average Test Accuracy: {test_accuracy:.4f}")
    st.write(f"Average Training Loss: {train_loss:.6f}")
    st.write(f"Average Test Loss: {test_loss:.6f}")
    st.write(f'\nDuration: {time.time() - start_time:.0f} seconds')
    st.write("Training completed!")

    # Plot the loss and accuracy for each iteration
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(train_correct, label='Training Accuracy')
    plt.xlabel('Iteration')
    plt.ylabel('Value')
    plt.title('Training Loss and Accuracy')
    plt.legend()

    # Display the plot using st.pyplot() instead of plt.show()
    st.pyplot()

    return model

def evaluate_model(model, test_loader, criterion, train_on_gpu):
    model.eval()

    test_loss = 0.0
    correct = 0

    with torch.no_grad():
        for X_test, y_test in test_loader:
            if train_on_gpu:
                X_test, y_test = X_test.cuda(), y_test.cuda()

            output = model(X_test)
            loss = criterion(output, y_test)
            test_loss += loss.item()

            # Calculate accuracy
            _, predicted = torch.max(output, 1)
            correct += (predicted == y_test).sum().item()

    test_loss /= len(test_loader.dataset)
    test_acc = correct / len(test_loader.dataset)

    return test_loss, test_acc

def show_confusion_matrix(test_loader, model):
    model.eval()

    # Check if a GPU is available
    train_on_gpu = torch.cuda.is_available()
    if train_on_gpu:
        model.cuda()

    true_labels = []
    predicted_labels = []

    with torch.no_grad():
        for X_test, y_test in test_loader:
            if train_on_gpu:
                X_test, y_test = X_test.cuda(), y_test.cuda()

            output = model(X_test)
            _, predicted = torch.max(output, 1)

            true_labels.extend(y_test.cpu().numpy())
            predicted_labels.extend(predicted.cpu().numpy())

    # Calculate the confusion matrix
    cm = confusion_matrix(true_labels, predicted_labels)
    class_labels = [str(i) for i in range(10)]

    # Display the confusion matrix using ConfusionMatrixDisplay
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)

    # Create a figure and axis explicitly
    fig, ax = plt.subplots(figsize=(10, 8))
    disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')

    # Display the plot using st.pyplot()
    st.pyplot(fig)

    return cm, class_labels

if __name__ == '__main__':
    main()
